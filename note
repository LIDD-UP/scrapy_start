scrapy 的使用方式，
安装：
    1：windows下：由于twisted模块有问题，直接去非官方python库去安装
    2：然后直接pip install scrapy
    3：需要用到pywin32；安装
demo
    1：scrapy startproject mySpider
    2:cd mySpider/spiders
    3:scrapy genspider itcast "itcast.cn"
    最终的结果是：
        name = 'itcast'
        allowed_domains = ['itcast.cn']
        start_urls = ['http://www.itcast.cn/']


scrapy 文件的集中保存形式 通过 -o来表示
# json格式，默认为Unicode编码
scrapy crawl itcast -o teachers.json

# json lines格式，默认为Unicode编码
scrapy crawl itcast -o teachers.jsonl

# csv 逗号表达式，可用Excel打开
scrapy crawl itcast -o teachers.csv

# xml格式
scrapy crawl itcast -o teachers.xml


Spider的作用；就是定义爬取网页的策略以及分析网页；
    类定义了如何爬取某个(或某些)网站。
    包括了爬取的动作(例如:是否跟进链接)以及如何从网页的内容中提取结构化数据(爬取item)。
    换句话说，Spider就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。






注意pymongo和mysql的连接数据库以及插入数据的区别：
host = settings['MONGODB_HOST']
        port = settings['MONGODB_PORT']
        dbname = settings['MONGODB_DBNAME']

        # pymongo.MongoClient(host, port) 创建MongoDB链接
        client = pymongo.MongoClient(host=host,port=port)

        # 指向指定的数据库
        mdb = client[dbname]
        # 获取数据库里存放数据的表名
        self.post = mdb[settings['MONGODB_DOCNAME']]


    def process_item(self, item, spider):
        data = dict(item)
        # 向指定的表里添加数据
        self.post.insert(data)
        return item